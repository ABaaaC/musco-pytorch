{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from model_utils import load_model\n",
    "\n",
    "MODEL_NAME = 'vgg16_imagenet'\n",
    "# MODEL_NAME = 'resnet50_imagenet'\n",
    "\n",
    "model = load_model(MODEL_NAME)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'student'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-6600039c43f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mPATH_TO_MODELS\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m\"/workspace/raid/data/eponomarev/pretrained\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_MODELS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/cifar10/compressed_models/vgg.vgg19_[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None].pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_fd\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(f, map_location, pickle_module)\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mdeserialized_storage_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'student'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "PATH_TO_MODELS =  \"/workspace/raid/data/eponomarev/pretrained\"\n",
    "\n",
    "model = torch.load(PATH_TO_MODELS+'/cifar10/compressed_models/vgg.vgg19_[None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None].pth')\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building cifar10 data loader with 4 workers\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import dataloaders\n",
    "\n",
    "bs = 1\n",
    "\n",
    "DATASET = 'cifar10'\n",
    "DATA_ROOT = '/workspace/raid/data/datasets/'\n",
    "loaders = dataloaders.get_loader(bs, DATASET, DATA_ROOT, num_workers = 4,\n",
    "                                 simple_normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get  all  layers\n",
    "\n",
    "Function  **get_layer_names()** returns names of model layers (convolutional and linear) and bool mask for convolutional layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1b96b8945f05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_layer_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mlayer_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layer_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_layer_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfc_layer_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mconv_layer_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from model_utils import get_layer_names\n",
    "\n",
    "layer_names, conv_layer_mask = get_layer_names(model)\n",
    "\n",
    "fc_layer_mask = (1 - conv_layer_mask).astype(bool)\n",
    "\n",
    "print(layer_names[conv_layer_mask])\n",
    "print(layer_names[fc_layer_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compress selected layers\n",
    "\n",
    "For **convolutional** layers\n",
    "- Set **decomposition**: 'tucker2', 'cp3' or 'cp4'\n",
    "- Set  decomposition **ranks** for convolutional layers (namely, ranks we use to decompose convolutional weight tensors). \n",
    "  - In Tucker2 case, for one layer \n",
    "      - If **rank = None**, the layer won't be decomposed.\n",
    "      - Elif **rank = 0**, then  VBMF method with **vbmf_weaken_factor**  will be used to select (rank_cout, rank_cin).\n",
    "      - Elif **rank = (-scalar) < 0**, then values (rank_cout, rank_cin) will be choosen as maximal values which allow **(sacalar x) layer parameter reduction**.\n",
    "      - Else **rank = tuple** and determines absolute ranks values (rank_cout, rank_cin)\n",
    "  - In CP case, rank for one layer is a scalar\n",
    "      - If **rank = None**, the layer won't be decomposed.\n",
    "      - Elif **rank = (-scalar) < 0** then value for rank will be choosen as maximal rank which allows **(sacalar x) layer parameter reduction**.\n",
    "      - Else **rank = scalar > 0** and determines absolute rank value.\n",
    "      \n",
    "For **linear** layers\n",
    "- Set **decomposition** = 'svd'\n",
    "- Set decomposition for linear layers (namely, ranks we use to factorize weight matrices)\n",
    "    - In SVD case, rank for one layer is a scalar\n",
    "      - If **rank = None**, the layer won't be decomposed.\n",
    "      - Elif **rank = 0**, then  VBMF method with **vbmf_weaken_factor**  will be used to select rank.\n",
    "      - Elif **rank = (-scalar) < 0** then value for rank will be choosen as maximal rank which allows **(sacalar x) layer parameter reduction**.\n",
    "      - Else **rank = scalar > 0** and determines absolute rank value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_resnet_layers_by_blocks(lnames):\n",
    "    starts = ['conv1'] + ['layer{}'.format(i) for i in range(1,5)]\n",
    "\n",
    "    start_idx = 0\n",
    "    blocks_idxs = []\n",
    "    layer_names_by_blocks = []\n",
    "\n",
    "    for s in starts:\n",
    "        curr_block =  [l for l in lnames if l.startswith(s)]\n",
    "        layer_names_by_blocks.append(curr_block)\n",
    "\n",
    "        blocks_idxs.append(np.arange(start_idx, start_idx+len(curr_block)))\n",
    "        start_idx += len(curr_block)\n",
    "\n",
    "    return blocks_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensor_compression import get_compressed_model\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# decomposition_conv = 'cp4'\n",
    "# decomposition_conv = 'cp3'\n",
    "decomposition_conv = 'tucker2'\n",
    "decomposition_fc = 'svd'\n",
    "\n",
    "RANK_SELECTION = 'vbmf'\n",
    "# RANK_SELECTION = 'nx'\n",
    "# RANK_SELECTION = 'custom'\n",
    "\n",
    "if RANK_SELECTION == 'vbmf':\n",
    "    WEAKEN_FACTOR = 0.9\n",
    "    X_FACTOR = 0\n",
    "    rank_selection_suffix = \"/wf:{}\".format(WEAKEN_FACTOR)\n",
    "elif RANK_SELECTION == 'nx':\n",
    "    WEAKEN_FACTOR = None  \n",
    "    X_FACTOR = 5\n",
    "    rank_selection_suffix = \"/{}x\".format(X_FACTOR)\n",
    "    \n",
    "    \n",
    "if MODEL_NAME == 'vgg16_imagenet':\n",
    "    ranks_conv = [None] + [-X_FACTOR]*(len(layer_names[conv_layer_mask])-1)\n",
    "    ranks_fc = [-X_FACTOR]*(len(layer_names[fc_layer_mask]))\n",
    "elif MODEL_NAME == 'resnet50_imagenet':\n",
    "    ranks_conv = [None if not name.endswith('conv2') else -X_FACTOR\n",
    "                  for name in layer_names[conv_layer_mask]]\n",
    "    ranks_fc = [-X_FACTOR]*(len(layer_names[fc_layer_mask]))\n",
    "    \n",
    "\n",
    "ranks = np.array([None]*len(layer_names))\n",
    "ranks[conv_layer_mask] = ranks_conv\n",
    "ranks[fc_layer_mask] = ranks_fc\n",
    "\n",
    "decompositions = np.array([None]*len(layer_names))\n",
    "decompositions[conv_layer_mask] = decomposition_conv\n",
    "decompositions[fc_layer_mask] = decomposition_fc\n",
    "\n",
    "CONV_SPLIT = 2\n",
    "FC_SPLIT = 1\n",
    "n_layers = len(layer_names)\n",
    "\n",
    "RESNET_SPLIT = True\n",
    "if MODEL_NAME == 'resnet50_imagenet' and RESNET_SPLIT:\n",
    "    split_tuples = split_resnet_layers_by_blocks(layer_names[conv_layer_mask])[::-1]\n",
    "else:\n",
    "    split_tuples = np.array_split(np.arange(n_layers)[conv_layer_mask], CONV_SPLIT)[::-1]\n",
    "split_tuples.append(np.array_split(np.arange(n_layers)[fc_layer_mask], FC_SPLIT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['features.17' 'features.19' 'features.21' 'features.24' 'features.26'\n",
      " 'features.28'] [0 0 0 0 0 0]\n",
      "Decompose layer features.17\n",
      "\t new rank:  [169, 124]\n",
      "Decompose layer features.19\n",
      "\t new rank:  [175, 184]\n",
      "Decompose layer features.21\n",
      "\t new rank:  [160, 170]\n",
      "Decompose layer features.24\n",
      "\t new rank:  [180, 192]\n",
      "Decompose layer features.26\n",
      "\t new rank:  [183, 185]\n",
      "Decompose layer features.28\n",
      "\t new rank:  [188, 190]\n",
      "['features.0' 'features.2' 'features.5' 'features.7' 'features.10'\n",
      " 'features.12' 'features.14'] [None 0 0 0 0 0 0]\n",
      "Skip layer features.0\n",
      "Decompose layer features.2\n",
      "\t new rank:  [29, 32]\n",
      "Decompose layer features.5\n",
      "\t new rank:  [47, 35]\n",
      "Decompose layer features.7\n",
      "\t new rank:  [50, 51]\n",
      "Decompose layer features.10\n",
      "\t new rank:  [89, 65]\n",
      "Decompose layer features.12\n",
      "\t new rank:  [95, 98]\n",
      "Decompose layer features.14\n",
      "\t new rank:  [88, 91]\n",
      "['classifier.0' 'classifier.3' 'classifier.6'] [0 0 0]\n",
      "Decompose layer classifier.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:3: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t new rank:  947\n",
      "Decompose layer classifier.3\n",
      "\t new rank:  634\n",
      "Decompose layer classifier.6\n",
      "\t new rank:  261\n"
     ]
    }
   ],
   "source": [
    "compressed_model = copy.deepcopy(model)\n",
    "for tupl in split_tuples:\n",
    "    lname, rank, decomposition = layer_names[tupl], ranks[tupl], decompositions[tupl]\n",
    "    print(lname, rank)\n",
    "    compressed_model = get_compressed_model(compressed_model,\n",
    "                                            ranks=rank,\n",
    "                                            layer_names=lname,\n",
    "                                            decompositions = decomposition,\n",
    "                                            vbmf_weaken_factor=WEAKEN_FACTOR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Sequential(\n",
       "      (2-0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (2-1): Conv2d(32, 29, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (2-2): Conv2d(29, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (3): ReLU(inplace)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Sequential(\n",
       "      (5-0): Conv2d(64, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (5-1): Conv2d(35, 47, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (5-2): Conv2d(47, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (6): ReLU(inplace)\n",
       "    (7): Sequential(\n",
       "      (7-0): Conv2d(128, 51, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (7-1): Conv2d(51, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (7-2): Conv2d(50, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (8): ReLU(inplace)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Sequential(\n",
       "      (10-0): Conv2d(128, 65, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (10-1): Conv2d(65, 89, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (10-2): Conv2d(89, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (11): ReLU(inplace)\n",
       "    (12): Sequential(\n",
       "      (12-0): Conv2d(256, 98, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (12-1): Conv2d(98, 95, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (12-2): Conv2d(95, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (13): ReLU(inplace)\n",
       "    (14): Sequential(\n",
       "      (14-0): Conv2d(256, 91, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (14-1): Conv2d(91, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (14-2): Conv2d(88, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (15): ReLU(inplace)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Sequential(\n",
       "      (17-0): Conv2d(256, 124, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (17-1): Conv2d(124, 169, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (17-2): Conv2d(169, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (18): ReLU(inplace)\n",
       "    (19): Sequential(\n",
       "      (19-0): Conv2d(512, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (19-1): Conv2d(184, 175, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (19-2): Conv2d(175, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (20): ReLU(inplace)\n",
       "    (21): Sequential(\n",
       "      (21-0): Conv2d(512, 170, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (21-1): Conv2d(170, 160, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (21-2): Conv2d(160, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (22): ReLU(inplace)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Sequential(\n",
       "      (24-0): Conv2d(512, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (24-1): Conv2d(192, 180, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (24-2): Conv2d(180, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (25): ReLU(inplace)\n",
       "    (26): Sequential(\n",
       "      (26-0): Conv2d(512, 185, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (26-1): Conv2d(185, 183, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (26-2): Conv2d(183, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (27): ReLU(inplace)\n",
       "    (28): Sequential(\n",
       "      (28-0): Conv2d(512, 190, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (28-1): Conv2d(190, 188, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (28-2): Conv2d(188, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (29): ReLU(inplace)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0-0): Linear(in_features=25088, out_features=947, bias=False)\n",
       "      (0-1): Linear(in_features=947, out_features=4096, bias=True)\n",
       "    )\n",
       "    (1): ReLU(inplace)\n",
       "    (2): Dropout(p=0.5)\n",
       "    (3): Sequential(\n",
       "      (3-0): Linear(in_features=4096, out_features=634, bias=False)\n",
       "      (3-1): Linear(in_features=634, out_features=4096, bias=True)\n",
       "    )\n",
       "    (4): ReLU(inplace)\n",
       "    (5): Dropout(p=0.5)\n",
       "    (6): Sequential(\n",
       "      (6-0): Linear(in_features=4096, out_features=261, bias=False)\n",
       "      (6-1): Linear(in_features=261, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def count_params(model):\n",
    "    n_params = 0\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        n_params += param.numel()\n",
    "    return n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.7107591584709225"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_count_dict_m = count_params(model)\n",
    "params_count_dict_cm = count_params(compressed_model)\n",
    "\n",
    "params_count_dict_m / params_count_dict_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 7,  8,  9, 10, 11, 12]),\n",
       " array([0, 1, 2, 3, 4, 5, 6]),\n",
       " [array([13, 14, 15])]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
